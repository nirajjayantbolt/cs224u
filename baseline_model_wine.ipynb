{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to notebook\n",
    "\n",
    "First objective: Figure out a simpler way to read in the data instead of loading all of it in memory\n",
    "I guess 50MB might not be so bad for this? Maybe lets proceed for now without using readers\n",
    "I think I am going to follow the general structure of https://towardsdatascience.com/perfume-recommendations-using-natural-language-processing-ad3e6736074c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINE_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"wine-reviews\", \"1k_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=['description', 'variety']\n",
    "df = pd.read_csv(WINE_SRC_FILENAME, usecols=col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description         variety\n",
      "0  Aromas include tropical fruit, broom, brimston...     White Blend\n",
      "1  This is ripe and fruity, a wine that is smooth...  Portuguese Red\n",
      "2  Tart and snappy, the flavors of lime flesh and...      Pinot Gris\n",
      "3  Pineapple rind, lemon pith and orange blossom ...        Riesling\n",
      "4  Much like the regular bottling from 2012, this...      Pinot Noir\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\n",
      "aromas include tropical fruit, broom, brimstone dried herb. palate isn't overly expressive, offering unripened apple, citrus dried sage alongside brisk acidity.\n"
     ]
    }
   ],
   "source": [
    "# think about also adding specific stopwords for wines\n",
    "print(df.description[0])\n",
    "df['reviews_new'] = df.description.str.lower().apply(remove_stopwords)\n",
    "print(df.reviews_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-299d2bff3ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Transform style_id products to document-term matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews_new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/tfidf_model.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1897\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mindices_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit TFIDF \n",
    "#Learn vocabulary and tfidf from all style_ids.\n",
    "tf = TfidfVectorizer(analyzer='word', \n",
    "                     min_df=10,\n",
    "                     ngram_range=(1, 1))\n",
    "tf.fit(df['reviews_new'])\n",
    "\n",
    "#Transform style_id products to document-term matrix.\n",
    "tfidf_matrix = tf.transform(df['reviews_new'])\n",
    "pickle.dump(tf, open(\"models/tfidf_model.pkl\", \"wb\"))\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the dimensionality of the matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "latent_matrix = svd.fit_transform(tfidf_matrix)\n",
    "pickle.dump(svd, open(\"models/svd_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129971, 25)\n"
     ]
    }
   ],
   "source": [
    "n = 25 #pick components\n",
    "#Use elbow and cumulative plot to pick number of components. \n",
    "#Need high ammount of variance explained. \n",
    "doc_labels = df.title\n",
    "svd_feature_matrix = pd.DataFrame(latent_matrix[:,0:n] ,index=doc_labels)\n",
    "print(svd_feature_matrix.shape)\n",
    "svd_feature_matrix.head()\n",
    "\n",
    "pickle.dump(svd_feature_matrix, open(\"models/lsa_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we are going to try and run the model just to see what interesting results we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pickle.load(open(\"models/tfidf_model.pkl\", \"rb\"))\n",
    "svd = pickle.load(open(\"models/svd_model.pkl\", \"rb\"))\n",
    "svd_feature_matrix = pickle.load(open(\"models/lsa_embeddings.pkl\", \"rb\"))\n",
    "\n",
    "def get_similarity_scores(message_array, embeddings):\n",
    "    cosine_sim_matrix = pd.DataFrame(cosine_similarity(X=embeddings,\n",
    "                                                       Y=message_array,\n",
    "                                                       dense_output=True))\n",
    "    cosine_sim_matrix.set_index(embeddings.index, inplace=True)\n",
    "    cosine_sim_matrix.columns = [\"cosine_similarity\"]\n",
    "    return cosine_sim_matrix\n",
    "\n",
    "def get_similarity(message):\n",
    "    message_array = tf.transform([message]).toarray()\n",
    "    message_array = svd.transform(message_array)\n",
    "    message_array = message_array[:,0:25].reshape(1, -1)\n",
    "    \n",
    "    bow_similarity = get_similarity_scores(message_array, svd_feature_matrix)\n",
    "    return bow_similarity\n",
    "\n",
    "def query_wines(message):\n",
    "    similar_wines = get_similarity(message)\n",
    "\n",
    "    return similar_wines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the perfume you are looking for. You can be as detailed as you like! \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc4702703164fa0b1d1feb101e71052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='citrus, white, green, bright')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c12cfe78ef247ccb016485c50cbe9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Restart!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got it! Hold tight while I find your recommendations!\n",
      "                                                    cosine_similarity\n",
      "title                                                                \n",
      "Nicosia 2013 Vulkà Bianco  (Etna)                            0.338419\n",
      "Quinta dos Avidagos 2011 Avidagos Red (Douro)               -0.020524\n",
      "Rainstorm 2013 Pinot Gris (Willamette Valley)                0.418326\n",
      "St. Julian 2013 Reserve Late Harvest Riesling (...           0.029545\n",
      "Sweet Cheeks 2012 Vintner's Reserve Wild Child ...           0.128369\n",
      "Tandem 2011 Ars In Vitro Tempranillo-Merlot (Na...           0.299210\n",
      "Terre di Giurfo 2013 Belsito Frappato (Vittoria)             0.486592\n",
      "Trimbach 2012 Gewurztraminer (Alsace)                        0.027913\n",
      "Heinz Eifel 2013 Shine Gewürztraminer (Rheinhes...           0.103685\n",
      "Jean-Baptiste Adam 2012 Les Natures Pinot Gris ...           0.323748\n",
      "Got it! Hold tight while I find your recommendations!\n",
      "                                                    cosine_similarity\n",
      "title                                                                \n",
      "Nicosia 2013 Vulkà Bianco  (Etna)                            0.338419\n",
      "Quinta dos Avidagos 2011 Avidagos Red (Douro)               -0.020524\n",
      "Rainstorm 2013 Pinot Gris (Willamette Valley)                0.418326\n",
      "St. Julian 2013 Reserve Late Harvest Riesling (...           0.029545\n",
      "Sweet Cheeks 2012 Vintner's Reserve Wild Child ...           0.128369\n",
      "Tandem 2011 Ars In Vitro Tempranillo-Merlot (Na...           0.299210\n",
      "Terre di Giurfo 2013 Belsito Frappato (Vittoria)             0.486592\n",
      "Trimbach 2012 Gewurztraminer (Alsace)                        0.027913\n",
      "Heinz Eifel 2013 Shine Gewürztraminer (Rheinhes...           0.103685\n",
      "Jean-Baptiste Adam 2012 Les Natures Pinot Gris ...           0.323748\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Describe the perfume you are looking for. You can be as detailed as you like! \")\n",
    "text = widgets.Text()\n",
    "display(text)\n",
    "button = widgets.Button(description=\"Restart!\")\n",
    "display(button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    clear_output()\n",
    "    print(\"Describe the perfume you are looking for. You can be as detailed as you like! \")\n",
    "    display(text)\n",
    "    display(button)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(\"Got it! Hold tight while I find your recommendations!\")\n",
    "    message = text.value\n",
    "    recs = query_wines(message)\n",
    "    print(recs)\n",
    "\n",
    "text.on_submit(handle_submit)\n",
    "button.on_click(on_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally new approach here, we are going to run a simple test with the contextual reps notebook and see how we do with a normal classifier by trying to follow along with the sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sst\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNClassifierModel\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_weights_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = BertTokenizer.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = BertModel.from_pretrained(hf_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_example_ids = hf_tokenizer.batch_encode_plus(\n",
    "    df.description, \n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'A',\n",
       " '##roma',\n",
       " '##s',\n",
       " 'include',\n",
       " 'tropical',\n",
       " 'fruit',\n",
       " ',',\n",
       " 'br',\n",
       " '##oom',\n",
       " ',',\n",
       " 'br',\n",
       " '##ims',\n",
       " '##tone',\n",
       " 'and',\n",
       " 'dried',\n",
       " 'herb',\n",
       " '.',\n",
       " 'The',\n",
       " 'p',\n",
       " '##ala',\n",
       " '##te',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'overly',\n",
       " 'expressive',\n",
       " ',',\n",
       " 'offering',\n",
       " 'un',\n",
       " '##rip',\n",
       " '##ened',\n",
       " 'apple',\n",
       " ',',\n",
       " 'c',\n",
       " '##itrus',\n",
       " 'and',\n",
       " 'dried',\n",
       " 'sage',\n",
       " 'alongside',\n",
       " 'br',\n",
       " '##isk',\n",
       " 'acid',\n",
       " '##ity',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(hf_example_ids['input_ids'][0])) # meaning there is a sentence with 163 BERT tokens\n",
    "hf_tokenizer.convert_ids_to_tokens(hf_example_ids['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hugging_face_bert_phi(description):\n",
    "    input_ids = hf_tokenizer.encode(description, add_special_tokens=True)\n",
    "    X = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        final_hidden_states, cls_output = hf_model(X)\n",
    "        return final_hidden_states.squeeze(0).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hugging_face_bert_classifier_phi(description):\n",
    "    reps = hugging_face_bert_phi(description)\n",
    "    #return reps.mean(axis=0)  # Another good, easy option.\n",
    "    return reps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('variety')\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make it 4000 for train, rest for dev\n",
    "training_size = 4000\n",
    "y_hf_train = y[:training_size]\n",
    "y_hf_dev = y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 27s, sys: 14.8 s, total: 21min 41s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "# Do not run these! Very expensive\n",
    "%time X_hf_train = [hugging_face_bert_classifier_phi(description) for description in X[:training_size].description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 1.83 s, total: 3min\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "# Do not run these! Very expensive\n",
    "%time X_hf_dev = [hugging_face_bert_classifier_phi(description) for description in X[training_size:].description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_mod = TorchShallowNeuralClassifier(max_iter=100, hidden_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 3.5912646055221558"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 2.7 s, total: 1min 20s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = hf_mod.fit(X_hf_train, y_hf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_preds = hf_mod.predict(X_hf_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                    Aglianico      0.000     0.000     0.000         3\n",
      "                       Albana      0.000     0.000     0.000         1\n",
      "                     Albariño      0.000     0.000     0.000         0\n",
      "                      Altesse      0.000     0.000     0.000         1\n",
      "                       Arinto      0.000     0.000     0.000         1\n",
      "                    Assyrtico      0.000     0.000     0.000         1\n",
      "                    Auxerrois      0.000     0.000     0.000         1\n",
      "                      Barbera      0.000     0.000     0.000         3\n",
      "                Blanc du Bois      0.000     0.000     0.000         1\n",
      "                      Bonarda      0.000     0.000     0.000         1\n",
      "     Bordeaux-style Red Blend      0.228     0.481     0.310        27\n",
      "   Bordeaux-style White Blend      0.000     0.000     0.000         3\n",
      "               Cabernet Blend      0.000     0.000     0.000         1\n",
      "               Cabernet Franc      0.000     0.000     0.000         8\n",
      "           Cabernet Sauvignon      0.122     0.136     0.129        44\n",
      "    Cabernet Sauvignon-Merlot      0.000     0.000     0.000         2\n",
      "                    Carmenère      0.500     0.250     0.333         4\n",
      "                   Carricante      0.000     0.000     0.000         0\n",
      "              Champagne Blend      0.182     0.333     0.235         6\n",
      "                   Chardonnay      0.362     0.300     0.328        70\n",
      "                 Chenin Blanc      0.000     0.000     0.000         4\n",
      "                    Colombard      0.000     0.000     0.000         1\n",
      "Corvina, Rondinella, Molinara      0.000     0.000     0.000         4\n",
      "                   Dornfelder      0.000     0.000     0.000         1\n",
      "                   Falanghina      0.000     0.000     0.000         1\n",
      "                        Fiano      0.000     0.000     0.000         1\n",
      "                        G-S-M      0.000     0.000     0.000         0\n",
      "                        Gamay      0.000     0.000     0.000         5\n",
      "                    Garganega      0.000     0.000     0.000         4\n",
      "                     Garnacha      0.000     0.000     0.000         2\n",
      "           Garnacha Tintorera      0.000     0.000     0.000         1\n",
      "               Gewürztraminer      0.000     0.000     0.000         1\n",
      "                        Glera      0.000     0.000     0.000         0\n",
      "                      Godello      0.000     0.000     0.000         1\n",
      "                        Greco      0.000     0.000     0.000         2\n",
      "                     Grenache      0.000     0.000     0.000         4\n",
      "                       Grillo      0.000     0.000     0.000         0\n",
      "             Grüner Veltliner      0.000     0.000     0.000         3\n",
      "                    Lambrusco      0.000     0.000     0.000         1\n",
      "                    Lemberger      0.000     0.000     0.000         1\n",
      "                       Malbec      0.200     0.111     0.143         9\n",
      "                     Malvasia      0.000     0.000     0.000         1\n",
      "                        Melon      0.000     0.000     0.000         2\n",
      "                       Mencía      0.000     0.000     0.000         1\n",
      "                     Meritage      0.000     0.000     0.000         0\n",
      "                       Merlot      0.167     0.071     0.100        14\n",
      "                   Monastrell      1.000     1.000     1.000         1\n",
      "                     Mondeuse      0.000     0.000     0.000         1\n",
      "                Montepulciano      0.000     0.000     0.000         1\n",
      "                     Moscatel      0.000     0.000     0.000         2\n",
      "                    Mourvèdre      0.000     0.000     0.000         1\n",
      "                       Muskat      0.000     0.000     0.000         1\n",
      "                     Nebbiolo      0.333     0.400     0.364        10\n",
      "            Nerello Mascalese      0.000     0.000     0.000         3\n",
      "                 Nero d'Avola      0.000     0.000     0.000         0\n",
      "                      Nosiola      0.000     0.000     0.000         1\n",
      "                     Ojaleshi      0.000     0.000     0.000         1\n",
      "                Pedro Ximénez      0.000     0.000     0.000         1\n",
      "                    Perricone      0.000     0.000     0.000         1\n",
      "                 Petite Sirah      0.000     0.000     0.000         4\n",
      "                 Pinot Grigio      0.091     1.000     0.167         1\n",
      "                   Pinot Gris      0.000     0.000     0.000         1\n",
      "                   Pinot Nero      0.000     0.000     0.000         0\n",
      "                   Pinot Noir      0.431     0.306     0.358        72\n",
      "                  Plavac Mali      0.000     0.000     0.000         1\n",
      "                         Port      0.000     0.000     0.000         4\n",
      "               Portuguese Red      0.273     0.250     0.261        12\n",
      "             Portuguese White      0.000     0.000     0.000         7\n",
      "                     Prosecco      0.000     0.000     0.000         0\n",
      "           Provence red blend      0.000     0.000     0.000         1\n",
      "         Provence white blend      0.000     0.000     0.000         2\n",
      "                    Red Blend      0.300     0.333     0.316        45\n",
      "        Rhône-style Red Blend      0.000     0.000     0.000         4\n",
      "      Rhône-style White Blend      0.000     0.000     0.000         2\n",
      "                     Riesling      0.259     0.583     0.359        12\n",
      "                       Rosato      0.000     0.000     0.000         0\n",
      "                         Rosé      0.250     0.400     0.308        10\n",
      "           Roussanne-Viognier      0.000     0.000     0.000         1\n",
      "                   Sangiovese      0.118     0.222     0.154         9\n",
      "            Sangiovese Grosso      0.000     0.000     0.000         0\n",
      "                     Saperavi      0.000     0.000     0.000         1\n",
      "                    Sauvignon      0.000     0.000     0.000         1\n",
      "              Sauvignon Blanc      0.381     0.235     0.291        34\n",
      "     Sauvignon Blanc-Semillon      0.000     0.000     0.000         2\n",
      "                       Sherry      0.000     0.000     0.000         2\n",
      "                       Shiraz      0.000     0.000     0.000         1\n",
      "              Sparkling Blend      0.000     0.000     0.000        12\n",
      "                        Syrah      0.069     0.118     0.087        17\n",
      "                 Syrah-Merlot      0.000     0.000     0.000         1\n",
      "                     Sémillon      0.000     0.000     0.000         1\n",
      "                       Tannat      0.000     0.000     0.000         1\n",
      "                  Tempranillo      0.143     0.250     0.182         4\n",
      "            Tempranillo Blend      0.000     0.000     0.000         3\n",
      "                Tinta de Toro      0.000     0.000     0.000         1\n",
      "                    Torrontés      0.000     0.000     0.000         1\n",
      "             Touriga Nacional      0.000     0.000     0.000         1\n",
      "                    Trebbiano      0.000     0.000     0.000         0\n",
      "                      Verdejo      0.000     0.000     0.000         1\n",
      "                Verdejo-Viura      0.000     0.000     0.000         1\n",
      "                   Vermentino      0.000     0.000     0.000         1\n",
      "                     Viognier      0.000     0.000     0.000         5\n",
      "                        Viura      0.000     0.000     0.000         1\n",
      "                  White Blend      0.200     0.111     0.143         9\n",
      "                    Zinfandel      0.083     0.250     0.125         8\n",
      "\n",
      "                     accuracy                          0.210       563\n",
      "                    macro avg      0.055     0.069     0.055       563\n",
      "                 weighted avg      0.214     0.210     0.202       563\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_hf_dev, hf_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
